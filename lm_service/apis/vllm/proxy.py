# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the LM-Service project

import asyncio
import os
import time
import uuid
from collections.abc import AsyncGenerator, Mapping
from typing import Any, Optional, Union

import msgspec
import numpy as np
import zmq
import zmq.asyncio
from vllm.config import ModelConfig, VllmConfig
from vllm.engine.protocol import EngineClient
from vllm.inputs.data import PromptType
from vllm.inputs.preprocess import InputPreprocessor
from vllm.lora.request import LoRARequest
from vllm.outputs import CompletionOutput, PoolingRequestOutput, RequestOutput
from vllm.pooling_params import PoolingParams
from vllm.sampling_params import SamplingParams
from vllm.transformers_utils.tokenizer import AnyTokenizer
from vllm.utils import Device, get_ip, get_open_port
from lm_service.protocol.protocol import (
    ExitRequest,
    FailureResponse,
    GenerationRequest,
    GenerationResponse,
    HeartbeatRequest,
    HeartbeatResponse,
    MetricsRequest,
    MetricsResponse,
    RequestType,
    ResponseType,
    ServerType,
    ShutdownRequest,
    WorkerRegisterRequest,
)
from lm_service.request_stats import RequestStatsMonitor
from lm_service.routing_logic import (
    RoutingInterface,
    RandomRouter,
    RoundRobinRouter,
    LeastInFlightRouter,
)
from lm_service.service_discovery import HealthCheckServiceDiscovery
from lm_service.stats_loggers import MetricsReporter
import lm_service.envs as lm_service_envs
from lm_service.metastore_client.factory import (
    MetastoreClientFactory,
)
from lm_service.metastore_client.metastore_client_config import (
    MetastoreClientConfig,
    json_to_metastore_config,
)
from lm_service.utils import is_addr_ipv6

from lm_service.logger_utils import init_logger

logger = init_logger(__name__)

ROUTER_MAP = {
    "RandomRouter": RandomRouter,
    "RoundRobinRouter": RoundRobinRouter,
    "LeastInFlightRouter": LeastInFlightRouter,
}


class Proxy(EngineClient):
    """
    Proxy
    """

    def __init__(
        self,
        proxy_addr: Optional[str] = None,
        encode_addr_list: Optional[list[str]] = None,
        pd_addr_list: Optional[list[str]] = None,
        p_addr_list: Optional[list[str]] = None,
        d_addr_list: Optional[list[str]] = None,
        model_name: str = "",
        router: type[RoutingInterface] = RandomRouter,
        enable_health_monitor: bool = True,
        health_check_interval: float = 10.0,
        health_threshold: int = 3,
        transfer_protocol: Optional[str] = None,
        metastore_client_config: Optional[dict] = None,
    ):
        self.queues: dict[str, asyncio.Queue] = {}

        self.encoder = msgspec.msgpack.Encoder()
        self.transfer_protocol = (
            lm_service_envs.TRANSFER_PROTOCOL or transfer_protocol or "ipc"
        )
        self.ctx = zmq.asyncio.Context()
        self.encoder_addr_list: list[str] = []
        self.pd_addr_list: list[str] = []
        self.p_addr_list: list[str] = []
        self.d_addr_list: list[str] = []
        self.to_encode_sockets: dict[str, zmq.asyncio.Socket] = {}
        self.to_pd_sockets: dict[str, zmq.asyncio.Socket] = {}
        self.to_p_sockets: dict[str, zmq.asyncio.Socket] = {}
        self.to_d_sockets: dict[str, zmq.asyncio.Socket] = {}
        self.enable_health_monitor = enable_health_monitor
        self.health_check_interval = health_check_interval
        self.health_threshold = health_threshold
        self.output_handler: Optional[asyncio.Task] = None
        self.router = router
        self.is_pd_merged = True

        # Dummy: needed for EngineClient Protocol.
        self.model_config = ModelConfig(
            model=model_name,
            tokenizer=model_name,
            tokenizer_mode="auto",
            trust_remote_code=False,
            dtype="auto",
            task="generate",
            seed=42,
        )
        if (
            metastore_client_config is not None
            or lm_service_envs.LM_SERVICE_METASTORE_CLIENT is not None
        ):
            config: MetastoreClientConfig = json_to_metastore_config(
                metastore_client_config
            )
            local_ip = get_ip()
            proxy_port = (
                int(lm_service_envs.LM_SERVICE_RPC_PORT)
                if lm_service_envs.LM_SERVICE_RPC_PORT
                else get_open_port()
            )
            proxy_addr = f"{local_ip}:{proxy_port}"
            self.proxy_addr = f"{self.transfer_protocol}://{proxy_addr}"
            if is_addr_ipv6(proxy_addr) and self.transfer_protocol == "tcp":
                self.ctx.setsockopt(zmq.constants.IPV6, 1)
            self.metastore_client = (
                MetastoreClientFactory.create_metastore_client(
                    config=config,
                    node_info=self.proxy_addr,
                    engine_type=ServerType.PROXY.value,
                    to_encode_sockets=self.to_encode_sockets,
                    to_pd_sockets=self.to_pd_sockets,
                    to_p_sockets=self.to_p_sockets,
                    to_d_sockets=self.to_d_sockets,
                )
            )
            self.is_pd_merged = self.metastore_client.is_pd_merge
        elif (
            proxy_addr is None
            or encode_addr_list is None
            or (
                pd_addr_list is None
                and (p_addr_list is None or d_addr_list is None)
            )
        ):
            raise ValueError(
                "proxy_addr, encode_addr_list, pd_addr_list must be provided"
            )
        else:
            if pd_addr_list is None:
                self.is_pd_merged = False
            self.proxy_addr = f"{self.transfer_protocol}://{proxy_addr}"
            if is_addr_ipv6(proxy_addr) and self.transfer_protocol == "tcp":
                self.ctx.setsockopt(zmq.constants.IPV6, 1)
            self.encoder_addr_list = [
                f"{self.transfer_protocol}://{addr}"
                for addr in encode_addr_list or []
            ]
            self.pd_addr_list = [
                f"{self.transfer_protocol}://{addr}"
                for addr in pd_addr_list or []
            ]
            self.p_addr_list = [
                f"{self.transfer_protocol}://{addr}"
                for addr in p_addr_list or []
            ]
            self.d_addr_list = [
                f"{self.transfer_protocol}://{addr}"
                for addr in d_addr_list or []
            ]
            self.to_encode_sockets = self.connect_to_socket(
                self.encoder_addr_list
            )
            self.to_pd_sockets = self.connect_to_socket(self.pd_addr_list)
            self.to_p_sockets = self.connect_to_socket(self.p_addr_list)
            self.to_d_sockets = self.connect_to_socket(self.d_addr_list)

        (
            self.encoder_service_discovery,
            self.encoder_metrics_logger,
            self.encoder_request_stats_monitor,
            self.encoder_router,
        ) = self._initialize_instance_clusters(
            ServerType.E_INSTANCE,
            self.to_encode_sockets,
        )

        if self.is_pd_merged:
            (
                self.pd_service_discovery,
                self.pd_metrics_logger,
                self.pd_request_stats_monitor,
                self.pd_router,
            ) = self._initialize_instance_clusters(
                ServerType.PD_INSTANCE, self.to_pd_sockets
            )
        else:
            (
                self.p_service_discovery,
                self.p_metrics_logger,
                self.p_request_stats_monitor,
                self.p_router,
            ) = self._initialize_instance_clusters(
                ServerType.P_INSTANCE, self.to_p_sockets
            )
            (
                self.d_service_discovery,
                self.d_metrics_logger,
                self.d_request_stats_monitor,
                self.d_router,
            ) = self._initialize_instance_clusters(
                ServerType.D_INSTANCE, self.to_d_sockets
            )

    def _initialize_instance_clusters(
        self,
        engine_type: ServerType,
        socket_dict: dict[str, zmq.asyncio.Socket],
    ) -> tuple[
        HealthCheckServiceDiscovery,
        MetricsReporter,
        RequestStatsMonitor,
        RoutingInterface,
    ]:
        service_discovery = HealthCheckServiceDiscovery(
            server_type=engine_type,
            instances=socket_dict,
            enable_health_monitor=self.enable_health_monitor,
            health_check_interval=self.health_check_interval,
            health_threshold=self.health_threshold,
            health_check_func=self.check_health,
        )
        metrics_logger = MetricsReporter(
            server_type=engine_type,
            instances=socket_dict,
            get_metrics_func=self.get_metrics,
        )
        request_stats_monitor = RequestStatsMonitor(socket_dict)
        route_policy = f"LM_SERVICE_{engine_type.name}_ROUTER"
        instance_router = (
            ROUTER_MAP.get(getattr(lm_service_envs, route_policy), None)
            or self.router
        )()
        return (
            service_discovery,
            metrics_logger,
            request_stats_monitor,
            instance_router,
        )

    def shutdown(self):
        self.ctx.destroy()
        if (task := self.output_handler) is not None:
            task.cancel()

        socket_path = self.proxy_addr.replace(
            f"{self.transfer_protocol}://", ""
        )
        if self.transfer_protocol == "ipc" and os.path.exists(socket_path):
            os.remove(socket_path)

    async def log_metrics(self) -> None:
        if self.is_pd_merged:
            await self.pd_metrics_logger.get_metrics()
        else:
            await self.p_metrics_logger.get_metrics()
            await self.d_metrics_logger.get_metrics()

        await self.encoder_metrics_logger.get_metrics()

    def connect_to_socket(
        self, addr_list: list[str]
    ) -> dict[str, zmq.asyncio.Socket]:
        """
        Connect to a list of ZMQ PUSH sockets.

        Args:
            addr_list: A list of ZMQ socket addresses to connect to.

        Returns:
            A dict of connected ZMQ PUSH sockets, with addr as key.
        """
        to_sockets = {}
        for addr in addr_list:
            socket = self.ctx.socket(zmq.constants.PUSH)
            socket.connect(addr)
            to_sockets[addr] = socket
            logger.info(f"Connected to worker {addr} success")
        return to_sockets

    async def _run_encode(
        self,
        request: GenerationRequest,
        q: asyncio.Queue[Union[Exception, GenerationResponse]],
    ) -> None:
        """
        Send the encode request to one encoder worker.
        The encoder worker is selected based on hashing the request ID.
        """
        if not self.to_encode_sockets:
            raise RuntimeError(
                "No encode workers configured: encode_addr_list is empty."
            )

        try:
            payload = self.encoder.encode(request)
        except Exception as e:
            raise RuntimeError("Failed to serialize GenerationRequest") from e

        msg = (RequestType.ENCODE, payload)
        health_endpoints = self.encoder_service_discovery.get_health_endpoints()
        request_stats = self.encoder_request_stats_monitor.get_request_stats()
        addr = self.encoder_router.route_request(
            health_endpoints, request_stats
        )
        self.encoder_request_stats_monitor.on_new_request(
            addr, request_id=request.request_id
        )
        try:
            socket = self.to_encode_sockets[addr]
            if lm_service_envs.TIMECOUNT_ENABLED:
                proxy_to_encode_time_start = time.perf_counter()
            await socket.send_multipart(msg, copy=False)
            response = await self._await_with_timeout(request.request_id, q)
            if (
                lm_service_envs.TIMECOUNT_ENABLED
                and isinstance(response, GenerationResponse)
                and response.proxy_to_worker_time_end
            ):
                self.encoder_metrics_logger.add_proxy_to_instance_time(
                    addr,
                    response.proxy_to_worker_time_end
                    - proxy_to_encode_time_start,
                )

            if isinstance(response, Exception):
                raise response
        finally:
            self.encoder_request_stats_monitor.on_request_completed(
                addr, request_id=request.request_id
            )

    async def _run_pd(
        self,
        request: GenerationRequest,
        q: asyncio.Queue[Union[Exception, GenerationResponse]],
    ):
        """
        Send the generation request to a PD worker and yield its response.
        The PD worker is selected based on hashing the request ID.
        """
        if not self.to_pd_sockets:
            raise RuntimeError(
                "No PD workers configured: pd_addr_list is empty."
            )

        try:
            payload = self.encoder.encode(request)
        except Exception as e:
            raise RuntimeError("Failed to serialize GenerationRequest") from e

        msg = (RequestType.GENERATION, payload)
        health_endpoints = self.pd_service_discovery.get_health_endpoints()
        request_stats = self.pd_request_stats_monitor.get_request_stats()
        addr = self.pd_router.route_request(health_endpoints, request_stats)
        self.pd_request_stats_monitor.on_new_request(
            addr, request_id=request.request_id
        )

        try:
            socket = self.to_pd_sockets[addr]
            if lm_service_envs.TIMECOUNT_ENABLED:
                proxy_to_pd_time_start = time.perf_counter()
            await socket.send_multipart(msg, copy=False)
            finished = False
            while not finished:
                response = await self._await_with_timeout(request.request_id, q)
                if isinstance(response, Exception):
                    raise response
                if (
                    lm_service_envs.TIMECOUNT_ENABLED
                    and isinstance(response, GenerationResponse)
                    and response.proxy_to_worker_time_end
                ):
                    self.pd_metrics_logger.add_proxy_to_instance_time(
                        addr,
                        response.proxy_to_worker_time_end
                        - proxy_to_pd_time_start,  # type: ignore
                    )
                finished = response.finish_reason is not None
                yield response
        finally:
            self.pd_request_stats_monitor.on_request_completed(
                addr, request_id=request.request_id
            )

    async def _run_prefill(
        self,
        request: GenerationRequest,
        q: asyncio.Queue[Union[Exception, GenerationResponse]],
    ):
        """
        Send the prefill request to one encoder worker.
        """
        if not self.to_p_sockets:
            raise RuntimeError(
                "No Prefill workers configured: p_addr_list is empty."
            )

        try:
            payload = self.encoder.encode(request)
        except Exception as e:
            raise RuntimeError("Failed to serialize GenerationRequest") from e

        msg = (RequestType.PREFILL, payload)
        health_endpoints = self.p_service_discovery.get_health_endpoints()
        request_stats = self.p_request_stats_monitor.get_request_stats()
        addr = self.p_router.route_request(health_endpoints, request_stats)
        self.p_request_stats_monitor.on_new_request(
            addr, request_id=request.request_id
        )

        try:
            socket = self.to_p_sockets[addr]
            if lm_service_envs.TIMECOUNT_ENABLED:
                proxy_to_p_time_start = time.perf_counter()
            await socket.send_multipart(msg, copy=False)
            response = await self._await_with_timeout(request.request_id, q)
            if (
                lm_service_envs.TIMECOUNT_ENABLED
                and isinstance(response, GenerationResponse)
                and response.proxy_to_worker_time_end
            ):
                self.p_metrics_logger.add_proxy_to_instance_time(
                    addr,
                    response.proxy_to_worker_time_end - proxy_to_p_time_start,  # type: ignore
                )

            if isinstance(response, Exception):
                raise response
        finally:
            self.p_request_stats_monitor.on_request_completed(
                addr, request_id=request.request_id
            )

    async def _run_decode(
        self,
        request: GenerationRequest,
        q: asyncio.Queue[Union[Exception, GenerationResponse]],
    ):
        """
        Send the generation request to a decode worker and yield its response.
        """
        if not self.to_d_sockets:
            raise RuntimeError(
                "No Decode workers configured: d_addr_list is empty."
            )

        try:
            payload = self.encoder.encode(request)
        except Exception as e:
            raise RuntimeError("Failed to serialize GenerationRequest") from e

        msg = (RequestType.GENERATION, payload)
        health_endpoints = self.d_service_discovery.get_health_endpoints()
        request_stats = self.d_request_stats_monitor.get_request_stats()
        addr = self.d_router.route_request(health_endpoints, request_stats)
        self.d_request_stats_monitor.on_new_request(
            addr, request_id=request.request_id
        )

        try:
            socket = self.to_d_sockets[addr]
            if lm_service_envs.TIMECOUNT_ENABLED:
                proxy_to_d_time_start = time.perf_counter()
            await socket.send_multipart(msg, copy=False)
            finished = False
            while not finished:
                response = await self._await_with_timeout(request.request_id, q)
                if isinstance(response, Exception):
                    raise response
                if (
                    lm_service_envs.TIMECOUNT_ENABLED
                    and isinstance(response, GenerationResponse)
                    and response.proxy_to_worker_time_end
                ):
                    self.d_metrics_logger.add_proxy_to_instance_time(
                        addr,
                        response.proxy_to_worker_time_end
                        - proxy_to_d_time_start,  # type: ignore
                    )
                finished = response.finish_reason is not None
                yield response
        finally:
            self.d_request_stats_monitor.on_request_completed(
                addr, request_id=request.request_id
            )

    def _to_request_output(self, resp: GenerationResponse) -> RequestOutput:
        """Convert a PD/Generate response to vLLM RequestOutput.

        This creates a single CompletionOutput. If the response includes
        text/token_ids attributes, they are used; otherwise defaults are used.
        """
        text = getattr(resp, "text", "")
        token_ids = getattr(resp, "token_ids", [])

        completion = CompletionOutput(
            index=0,
            text=text,
            token_ids=token_ids,
            cumulative_logprob=None,
            logprobs=None,
            finish_reason=resp.finish_reason,
            stop_reason=resp.stop_reason,
        )

        return RequestOutput(
            request_id=resp.request_id,
            prompt=None,
            prompt_token_ids=resp.prompt_token_ids,
            prompt_logprobs=None,
            outputs=[completion],
            finished=resp.finish_reason is not None,
        )

    async def generate(
        self,
        prompt: PromptType,
        sampling_params: SamplingParams,
        request_id: Optional[str] = None,
        lora_request: Optional[LoRARequest] = None,
        trace_headers: Optional[Mapping[str, str]] = None,
        priority: int = 0,
    ):
        # lazy initialization
        if self.output_handler is None:
            self.output_handler = asyncio.create_task(
                self._run_output_handler()
            )
        if self.encoder_service_discovery.should_launch_health_monitor():
            self.encoder_service_discovery.launch_health_monitor()
        # PD-merged or not
        if self.is_pd_merged:
            if self.pd_service_discovery.should_launch_health_monitor():
                self.pd_service_discovery.launch_health_monitor()
        else:
            if self.p_service_discovery.should_launch_health_monitor():
                self.p_service_discovery.launch_health_monitor()
            if self.d_service_discovery.should_launch_health_monitor():
                self.d_service_discovery.launch_health_monitor()

        if not request_id:
            request_id = uuid.uuid4().hex

        q: asyncio.Queue = asyncio.Queue()
        if request_id in self.queues:
            raise ValueError(f"Request id {request_id} already running.")
        else:
            self.queues[request_id] = q

        # Support both raw string prompts and dict prompts with multimodal data
        prompt_text = prompt["prompt"] if isinstance(prompt, dict) else prompt

        request = GenerationRequest(
            request_id=request_id,
            prompt=prompt_text,
            sampling_params=sampling_params,
            proxy_addr=self.proxy_addr,
        )

        try:
            proxy_ttft_start: float = time.perf_counter()
            ttft_recorded_flag: bool = False
            # need to validate to avoid decode failed later
            req_dict = msgspec.to_builtins(request)
            request = msgspec.convert(req_dict, GenerationRequest, strict=True)

            if _has_mm_data(prompt):
                request.multi_modal_data = _encode_mm_data(
                    prompt["multi_modal_data"]
                )
                await self._run_encode(request, q)

            if self.is_pd_merged:
                async for pd_response in self._run_pd(request, q):
                    yield self._to_request_output(pd_response)
                    ttft_recorded_flag = self.pd_metrics_logger.cal_proxy_ttft(
                        ttft_recorded_flag,
                        proxy_ttft_start,
                        pd_response,
                    )
            else:
                await self._run_prefill(request, q)
                async for d_response in self._run_decode(request, q):
                    yield self._to_request_output(d_response)
                    ttft_recorded_flag = self.d_metrics_logger.cal_proxy_ttft(
                        ttft_recorded_flag,
                        proxy_ttft_start,
                        d_response,
                    )

        except msgspec.ValidationError as e:
            raise RuntimeError(f"Invalid Parameters: {e}.") from e
        finally:
            self.queues.pop(request_id, None)

    async def abort_requests_from_unhealth_endpoints(
        self, server_type, unhealth_endpoints, request_stats_monitor
    ) -> None:
        request_stats = request_stats_monitor.get_request_stats()

        async def fail_request(req_id, iid):
            if req_id in self.queues:
                await self.queues[req_id].put(
                    RuntimeError(
                        f"{server_type} instance {iid} is unhealthy, "
                        f"so abort its request {req_id}."
                    )
                )

        tasks = [
            asyncio.create_task(fail_request(req_id, iid))
            for iid in unhealth_endpoints
            for req_id in request_stats.get(iid).in_flight_requests
        ]

        await asyncio.gather(*tasks, return_exceptions=True)

    def add_unhealthy_task(
        self,
        engine_type: ServerType,
        discovery: HealthCheckServiceDiscovery,
        request_stats_monitor: RequestStatsMonitor,
        tasks: list[Any],
    ) -> None:
        unhealthy_endpoints = discovery.get_unhealth_endpoints()
        if unhealthy_endpoints:
            tasks.append(
                self.abort_requests_from_unhealth_endpoints(
                    server_type=engine_type,
                    unhealth_endpoints=unhealthy_endpoints,
                    request_stats_monitor=request_stats_monitor,
                )
            )

    async def _worker_register_handler(
        self, worker_register_req: WorkerRegisterRequest
    ):
        """Handle worker register request."""
        address = worker_register_req.address
        server_type = worker_register_req.server_type

        SERVER_TYPE_TO_SOCKET_MAP = {
            ServerType.E_INSTANCE: self.to_encode_sockets,
            ServerType.PD_INSTANCE: self.to_pd_sockets,
            ServerType.P_INSTANCE: self.to_p_sockets,
            ServerType.D_INSTANCE: self.to_d_sockets,
        }
        if server_type in SERVER_TYPE_TO_SOCKET_MAP:
            socket_dict = SERVER_TYPE_TO_SOCKET_MAP[server_type]
            if address not in socket_dict:
                try:
                    socket = self.ctx.socket(zmq.constants.PUSH)
                    socket.connect(address)
                    socket_dict[address] = socket
                except zmq.ZMQError as e:
                    logger.error(
                        f"Failed to connect to worker {address} with error: {e}"
                    )
        else:
            logger.error(
                f"_worker_register_handler fail, unknown server type {server_type}"
            )
            return

        logger.info(f"Connected to worker {address} success")

    async def _run_output_handler(self) -> None:
        """Background task to pull responses and dispatch to request queues.

        Binds a PULL socket on proxy_addr and receives multipart messages of
        the form (response_type, payload). Decodes payload into a
        GenerationResponse and enqueues it into the corresponding request queue
        keyed by request_id.
        """
        socket: Optional[zmq.asyncio.Socket] = None
        decoder = msgspec.msgpack.Decoder(GenerationResponse)
        failure_decoder = msgspec.msgpack.Decoder(FailureResponse)
        heartbeat_decoder = msgspec.msgpack.Decoder(HeartbeatResponse)
        metrics_decoder = msgspec.msgpack.Decoder(MetricsResponse)
        sigterm_decoder = msgspec.msgpack.Decoder(ShutdownRequest)
        worker_register_decoder = msgspec.msgpack.Decoder(WorkerRegisterRequest)
        try:
            socket = self.ctx.socket(zmq.constants.PULL)
            socket.bind(self.proxy_addr)
            timeout = self.health_check_interval * self.health_threshold / 2

            while True:
                tasks: list[asyncio.Task] = []
                self.add_unhealthy_task(
                    engine_type=ServerType.E_INSTANCE,
                    discovery=self.encoder_service_discovery,
                    request_stats_monitor=self.encoder_request_stats_monitor,
                    tasks=tasks,
                )
                if self.is_pd_merged:
                    self.add_unhealthy_task(
                        engine_type=ServerType.PD_INSTANCE,
                        discovery=self.pd_service_discovery,
                        request_stats_monitor=self.pd_request_stats_monitor,
                        tasks=tasks,
                    )
                else:
                    self.add_unhealthy_task(
                        engine_type=ServerType.P_INSTANCE,
                        discovery=self.p_service_discovery,
                        request_stats_monitor=self.p_request_stats_monitor,
                        tasks=tasks,
                    )
                    self.add_unhealthy_task(
                        engine_type=ServerType.D_INSTANCE,
                        discovery=self.d_service_discovery,
                        request_stats_monitor=self.d_request_stats_monitor,
                        tasks=tasks,
                    )

                if tasks:
                    await asyncio.gather(*tasks, return_exceptions=True)

                # Check if the engine is alive:
                if not await socket.poll(timeout=timeout):
                    continue
                resp_type, payload = await socket.recv_multipart()

                # Decode response according to its type.
                # TODO : judge whether we need to add PREFILL response type
                resp: Union[
                    GenerationResponse,
                    HeartbeatResponse,
                    FailureResponse,
                    MetricsResponse,
                    ShutdownRequest,
                    WorkerRegisterRequest,
                ]
                # TODO: maybe we can have a mapping from resp_type to prefill
                if resp_type in (
                    ResponseType.GENERATION,
                    ResponseType.ENCODE,
                    ResponseType.PREFILL,
                ):
                    resp = decoder.decode(payload)
                elif resp_type == ResponseType.HEARTBEAT:
                    resp = heartbeat_decoder.decode(payload)
                elif resp_type == ResponseType.FAILURE:
                    resp = failure_decoder.decode(payload)
                elif resp_type == ResponseType.METRICS:
                    resp = metrics_decoder.decode(payload)
                elif resp_type == ResponseType.SIGTERM:
                    resp = sigterm_decoder.decode(payload)
                    task = asyncio.create_task(
                        self.handle_sigterm_from_worker(resp)
                    )
                    task.add_done_callback(
                        lambda t: logger.error(
                            "Exception in handle_sigterm_from_worker: %s",
                            t.exception(),
                        )
                        if t.exception() is not None and not t.cancelled()
                        else None
                    )
                elif resp_type == RequestType.REGISTER:
                    resp = worker_register_decoder.decode(payload)
                    asyncio.create_task(self._worker_register_handler(resp))
                else:
                    raise RuntimeError(
                        f"Unknown response type from worker: {resp_type.decode()}"
                    )

                if resp.request_id not in self.queues:
                    if resp_type not in (
                        ResponseType.HEARTBEAT,
                        ResponseType.METRICS,
                        ResponseType.SIGTERM,
                        RequestType.REGISTER,
                    ):
                        logger.warning(
                            "Request %s may have been aborted, ignore response.",
                            resp.request_id,
                        )
                elif isinstance(resp, FailureResponse):
                    self.queues[resp.request_id].put_nowait(
                        RuntimeError(f"Request error: {resp.error_message}")
                    )
                else:
                    self.queues[resp.request_id].put_nowait(resp)

        except Exception as e:
            # TODO: maybe there is a more fine-grained way to handle errors.
            # For now, if there is any error, we terminate all requests.
            for q in self.queues.values():
                q.put_nowait(e)
        finally:
            if socket is not None:
                socket.close(linger=0)

    def encode(
        self,
        prompt: PromptType,
        pooling_params: PoolingParams,
        request_id: str,
        lora_request: Optional[LoRARequest] = None,
        trace_headers: Optional[Mapping[str, str]] = None,
        priority: int = 0,
    ) -> AsyncGenerator[PoolingRequestOutput, None]:
        raise NotImplementedError

    async def abort(self, request_id: str) -> None:
        raise NotImplementedError

    async def get_vllm_config(self) -> VllmConfig:
        """Get the vllm configuration of the vLLM engine."""
        raise NotImplementedError

    async def get_model_config(self) -> ModelConfig:
        return self.model_config

    async def get_input_preprocessor(self) -> InputPreprocessor:
        raise NotImplementedError

    async def get_tokenizer(self) -> AnyTokenizer:
        raise NotImplementedError

    async def is_tracing_enabled(self) -> bool:
        return False

    async def do_log_stats(self) -> None:
        pass

    async def check_health(self, server_type: ServerType, addr: str):
        # lazy initialization
        if self.output_handler is None:
            self.output_handler = asyncio.create_task(
                self._run_output_handler()
            )
        request_id = str(uuid.uuid4())
        request = HeartbeatRequest(
            request_id=request_id, proxy_addr=self.proxy_addr
        )
        q: asyncio.Queue = asyncio.Queue()
        self.queues[request_id] = q
        try:
            payload = self.encoder.encode(request)
            msg = (RequestType.HEARTBEAT, payload)
            _, sockets = self._get_sockets_and_server_types_from_addr(
                addr, server_type
            )
            socket = sockets[addr]

            await socket.send_multipart(msg, copy=False)
            response = await q.get()
            if (
                isinstance(response, HeartbeatResponse)
                and response.status == "OK"
            ):
                return True
            elif isinstance(response, Exception):
                raise response
            else:
                return False

        except Exception as e:
            raise RuntimeError(
                f"Health check failed for {server_type} {addr}, exception: {e}"
            ) from e
        finally:
            self.queues.pop(request_id, None)

    async def get_metrics(self, server_type: ServerType, addr: str):
        request_id = str(uuid.uuid4())
        request = MetricsRequest(
            request_id=request_id, proxy_addr=self.proxy_addr
        )
        q: asyncio.Queue = asyncio.Queue()
        self.queues[request_id] = q
        try:
            payload = self.encoder.encode(request)
            msg = (RequestType.METRICS, payload)
            _, sockets = self._get_sockets_and_server_types_from_addr(
                addr, server_type
            )
            socket = sockets[addr]

            await socket.send_multipart(msg, copy=False)
            response = await q.get()
            # calculate proxy to pd/encode time
            if (
                isinstance(response, MetricsResponse)
                and response.metrics is not None
            ):
                # calculate proxy to pd/encode time average
                # add to metrics
                proxy_ttft_avg: float = 0.0
                if server_type == ServerType.E_INSTANCE:
                    proxy2instance_avg = self.encoder_metrics_logger.get_avg_proxy_to_instance_time(
                        addr
                    )
                elif server_type == ServerType.PD_INSTANCE:
                    proxy2instance_avg = (
                        self.pd_metrics_logger.get_avg_proxy_to_instance_time(
                            addr
                        )
                    )
                    proxy_ttft_avg = self.pd_metrics_logger.get_avg_proxy_ttft()
                elif server_type == ServerType.P_INSTANCE:
                    proxy2instance_avg = (
                        self.p_metrics_logger.get_avg_proxy_to_instance_time(
                            addr
                        )
                    )
                elif server_type == ServerType.D_INSTANCE:
                    proxy2instance_avg = (
                        self.d_metrics_logger.get_avg_proxy_to_instance_time(
                            addr
                        )
                    )
                    proxy_ttft_avg = self.d_metrics_logger.get_avg_proxy_ttft()
                for engine_id in response.metrics:
                    response.metrics[engine_id].update(
                        {
                            "proxy_to_instance_time_avg": proxy2instance_avg,  # type: ignore
                            "proxy_ttft_avg": proxy_ttft_avg,  # type: ignore
                        }
                    )

                return response.metrics
            elif isinstance(response, Exception):
                raise response
            else:
                return None

        except Exception as e:
            raise RuntimeError(
                "Get metrics failed for %s %s, exception: %s"
                % (server_type, addr, e)
            ) from e
        finally:
            self.queues.pop(request_id, None)

    async def exit_instance(
        self, addr: str, server_type: Optional[ServerType] = None
    ) -> None:
        """
        request the specified instance to exit gracefully:
        1. add the instance to the draining set (stop routing new requests)
        2. send EXIT request
        3. the instance will remove itself from service discovery
        """

        # lazy initialization
        if self.output_handler is None:
            self.output_handler = asyncio.create_task(
                self._run_output_handler()
            )
        if addr is None:
            logger.warning(
                "Exit instance failed, addr is None.",
            )
            return
        worker_addr = (
            f"{self.transfer_protocol}://{addr}"
            if not addr.startswith(self.transfer_protocol)
            else addr
        )
        server_type, sockets = self._get_sockets_and_server_types_from_addr(
            worker_addr, server_type
        )
        socket = sockets.get(worker_addr, None)
        if socket is None:
            logger.warning(
                "Exit instance failed for %s, addr %s not found.",
                server_type,
                worker_addr,
            )
            return
        # Create exit request
        request_id = str(uuid.uuid4())
        request = ExitRequest(request_id=request_id)
        try:
            payload = self.encoder.encode(request)
            msg = (RequestType.EXIT, payload)

            await socket.send_multipart(msg, copy=False)
            logger.info(
                "Exit request sent to %s instance addr %s.",
                server_type,
                addr,
            )
        except Exception as e:
            raise RuntimeError(
                "Exit instance failed, exception: %s" % (e)
            ) from e
        sockets.pop(worker_addr, None)  # stop routing new requests
        await self.refresh_health_status(worker_addr, server_type)
        node_key = (
            f"{lm_service_envs.LM_SERVICE_REDIS_KEY_PREFIX}_{server_type.name}"
        )
        if (
            lm_service_envs.LM_SERVICE_METASTORE_CLIENT is not None
            and hasattr(self, "metastore_client")
            and self.metastore_client is not None
            and hasattr(self.metastore_client, "delete_metadata")
        ):
            self.metastore_client.delete_metadata(node_key, worker_addr)

    async def handle_sigterm_from_worker(self, req: ShutdownRequest) -> None:
        # lazy initialization
        if self.output_handler is None:
            self.output_handler = asyncio.create_task(
                self._run_output_handler()
            )
        # find instance id by addr, stop routing new requests to it
        try:
            server_type, sockets = self._get_sockets_and_server_types_from_addr(
                req.addr
            )
        except ValueError:
            logger.warning(
                "Instance addr %s not found.",
                req.addr,
            )
            return
        sockets.pop(req.addr, None)  # stop routing new requests to it
        await self.refresh_health_status(req.addr, server_type)
        logger.info(
            "Instance %s addr %s is exiting (reason=%s, in_flight=%d).",
            server_type,
            req.addr,
            req.reason,
            req.in_flight,
        )

    async def _await_with_timeout(
        self,
        request_id: str,
        q: asyncio.Queue[Union[Exception, GenerationResponse]],
    ) -> Union[Exception, GenerationResponse]:
        """wait for response from queue with timeout handling."""
        try:
            resp = await asyncio.wait_for(
                q.get(),
                timeout=lm_service_envs.LM_SERVICE_REQUEST_TIMEOUT_SECONDS,
            )
            return resp
        except asyncio.TimeoutError:
            return RuntimeError(
                f"Request {request_id} timed out "
                f"after {lm_service_envs.LM_SERVICE_REQUEST_TIMEOUT_SECONDS}s "
                f"without worker response."
            )

    async def start_profile(self) -> None:
        raise NotImplementedError

    async def stop_profile(self) -> None:
        raise NotImplementedError

    async def reset_prefix_cache(self, device: Optional[Device] = None) -> None:
        raise NotImplementedError

    async def sleep(self, level: int = 1) -> None:
        raise NotImplementedError

    async def wake_up(self, tags: list[str] | None = None) -> None:
        raise NotImplementedError

    async def is_sleeping(self) -> bool:
        return False

    async def add_lora(self, lora_request: LoRARequest) -> bool:
        raise NotImplementedError

    @property
    def errored(self) -> bool:
        return False

    def dead_error(self) -> Exception:
        return Exception("PDController has failed.")

    def is_running(self) -> bool:
        return True

    def is_stopped(self) -> bool:
        return False

    async def reset_mm_cache(self) -> None:
        raise NotImplementedError

    def _get_sockets_and_server_types_from_addr(
        self, addr: str, server_type: Optional[ServerType] = None
    ) -> tuple[ServerType, dict[str, zmq.asyncio.Socket]]:
        sockets_dict = {
            ServerType.PD_INSTANCE: self.to_pd_sockets,
            ServerType.P_INSTANCE: self.to_p_sockets,
            ServerType.D_INSTANCE: self.to_d_sockets,
            ServerType.E_INSTANCE: self.to_encode_sockets,
        }
        if server_type is None:
            for stype, sockets in sockets_dict.items():
                if addr in sockets:
                    return stype, sockets
        else:
            sockets = sockets_dict.get(server_type, {})
            if addr in sockets:
                return server_type, sockets
        raise ValueError(
            f"Address {addr} not found in any server type sockets."
        )

    async def refresh_health_status(
            self, addr: str, server_type: ServerType
    ) -> None:
        if server_type == ServerType.E_INSTANCE:
            await self.encoder_service_discovery.refresh_health_status(addr)
        elif server_type == ServerType.PD_INSTANCE:
            await self.pd_service_discovery.refresh_health_status(addr)
        elif server_type == ServerType.P_INSTANCE:
            await self.p_service_discovery.refresh_health_status(addr)
        elif server_type == ServerType.D_INSTANCE:
            await self.d_service_discovery.refresh_health_status(addr)


def _has_mm_data(prompt: PromptType) -> bool:
    if isinstance(prompt, dict):
        return "multi_modal_data" in prompt
    return False


def _encode_mm_data(mm_data: dict[str, Any]) -> dict[str, Any]:
    images = mm_data.get("image", [])
    if not isinstance(images, list):
        images = [images]
    encoded_images = []
    for img in images:
        if isinstance(img, np.ndarray):
            encoded_img = {
                "type": "ndarray",
                "data": img.tobytes(),
                "shape": img.shape,
                "dtype": str(img.dtype),
            }
            encoded_images.append(encoded_img)
    return {"image": encoded_images}
